{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Movies Pipeline\n",
    "\n",
    "Note: based on this [Haystack Blog](https://haystack.deepset.ai/tutorials/27_first_rag_pipeline).\n",
    "\n",
    "We will implement a simple Haystack pipeline that implements RAG: data will be embedded and stored. Later it will be searched, retrieved and used by an LLM to answer questions. We will use text embedding OPEA component, as well as an OPEA generator, representing the LLM.\n",
    "\n",
    "Let's go, step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HryYZP9ZO-qb"
   },
   "source": [
    "### Downloading the data\n",
    "\n",
    "We download a [movie dataset](https://huggingface.co/datasets/facebook/wiki_movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INdC3WvLO-qb",
    "outputId": "1af43d0f-2999-4de4-d152-b3cca9fb49e6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from haystack import Document\n",
    "\n",
    "dataset = load_dataset(\"facebook/wiki_movies\", split=\"train\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lvfew16O-qa"
   },
   "source": [
    "## Document Store\n",
    "\n",
    "Initialize a DocumentStore to index your documents. We combine the question and answer in each example in our dataset to serve as our documents in the local document store. We first insert the text and then we add the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CbVN-s5LO-qa"
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INdC3WvLO-qb",
    "outputId": "1af43d0f-2999-4de4-d152-b3cca9fb49e6"
   },
   "outputs": [],
   "source": [
    "docs = [Document(content=f\"Q: {doc['question']} A: {doc['answer']}\") for doc in dataset.select(range(1000))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czMjWwnxPA-3"
   },
   "source": [
    "## Document Embedding\n",
    "\n",
    "We use the `OPEADocumentEmbedder` to convert each example to a vector and then store it in the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUmAH9sEn3R7",
    "outputId": "ee54b59b-4d4a-45eb-c1a9-0b7b248f1dd4"
   },
   "outputs": [],
   "source": [
    "from haystack_opea import OPEADocumentEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUmAH9sEn3R7",
    "outputId": "ee54b59b-4d4a-45eb-c1a9-0b7b248f1dd4"
   },
   "outputs": [],
   "source": [
    "doc_embedder = OPEADocumentEmbedder(\"http://localhost:6006/v1\")\n",
    "doc_embedder.warm_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7d482188c12d4a7886f20a65d3402c59",
      "2a3ec74419ae4a02ac0210db66133415",
      "ddeff9a822404adbbc3cad97a939bc0c",
      "36d341ab3a044709b5af2e8ab97559bc",
      "88fc33e1ab78405e911b5eafa512c935",
      "91e5d4b0ede848319ef0d3b558d57d19",
      "d2428c21707d43f2b6f07bfafbace8bb",
      "7fdb2c859e454e72888709a835f7591e",
      "6b8334e071a3438397ba6435aac69f58",
      "5f5cfa425cac4d37b2ea29e53b4ed900",
      "3c59a82dac5c476b9a3e3132094e1702"
     ]
    },
    "id": "ETpQKftLplqh",
    "outputId": "b9c8658c-90c8-497c-e765-97487c0daf8e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_with_embeddings = doc_embedder.run(docs)\n",
    "document_store.write_documents(docs_with_embeddings[\"documents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdojTxg6uubn"
   },
   "source": [
    "## Building the RAG Pipeline\n",
    "\n",
    "The pipeline will be comprised by an `OPEATextEmbedder` for converting the question into a vector to use for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LyJY2yW628dl"
   },
   "outputs": [],
   "source": [
    "from haystack_opea import OPEATextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LyJY2yW628dl"
   },
   "outputs": [],
   "source": [
    "text_embedder = OPEATextEmbedder(\"http://localhost:6006/v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_cj-5m-O-qb"
   },
   "source": [
    "## Retriever\n",
    "\n",
    "We use a simple in-memory retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-uo-6fjiO-qb"
   },
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "We introduce a prompt using the `PromptBuilder` component. Prompts are represented using jinja templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.builders.prompt_builder import PromptBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Given the following information, answer the question.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Question: {{question}}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_builder = PromptBuilder(template=prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM\n",
    "\n",
    "We use the `OPEAGenerator`. It requires an endpoint, and optionally a model name and model options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_opea import OPEAGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_opea import OPEAGenerator\n",
    "\n",
    "llm = OPEAGenerator(\n",
    "    \"http://localhost:9009/v1\",\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    model_arguments={\"max_tokens\": 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Pipeline\n",
    "\n",
    "Building a pipeline is done by defining the components and then defining the connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f6NFmpjEO-qb",
    "outputId": "89fd1b48-5189-4401-9cf8-15f55c503676"
   },
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "\n",
    "basic_rag_pipeline = Pipeline()\n",
    "\n",
    "basic_rag_pipeline.add_component(\"text_embedder\", text_embedder)\n",
    "basic_rag_pipeline.add_component(\"retriever\", retriever)\n",
    "basic_rag_pipeline.add_component(\"prompt_builder\", prompt_builder)\n",
    "basic_rag_pipeline.add_component(\"llm\", llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fb56ef5bb60>\n",
       "ðŸš… Components\n",
       "  - text_embedder: OPEATextEmbedder\n",
       "  - retriever: InMemoryEmbeddingRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OPEAGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - text_embedder.embedding -> retriever.query_embedding (List[float])\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_rag_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "basic_rag_pipeline.connect(\"retriever\", \"prompt_builder\")\n",
    "basic_rag_pipeline.connect(\"prompt_builder.prompt\", \"llm.prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBAyF5tVO-qc"
   },
   "source": [
    "When asking a question, use the `run()` method of the pipeline. Make sure to provide the question to both the `text_embedder` and the `prompt_builder`. This ensures that the `{{question}}` variable in the template prompt gets replaced with your specific question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "4e6e97b6d54f4f80bb7e8b25aba8e616",
      "1a820c06a7a049d8b6c9ff300284d06e",
      "58ff4e0603a74978a134f63533859be5",
      "8bdb8bfae31d4f4cb6c3b0bf43120eed",
      "39a68d9a5c274e2dafaa2d1f86eea768",
      "d0cfe5dacdfc431a91b4c4741123e2d0",
      "e7f1e1a14bb740d18827dd78bbe7b2e3",
      "3fda06f905b445a488efdd2dd08c0939",
      "2bc341a780f7498ba9cd475468841bb5",
      "d7218475e23b420a8c03d00ca4ab8718",
      "a694abaf765f4d1b82fa0138e59c6793"
     ]
    },
    "id": "Vnt283M5O-qc",
    "outputId": "d2843a73-3ad5-4daa-8d1e-a58de7aa2bb0"
   },
   "outputs": [],
   "source": [
    "question = \"What are some movies by Ridley Scott?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "4e6e97b6d54f4f80bb7e8b25aba8e616",
      "1a820c06a7a049d8b6c9ff300284d06e",
      "58ff4e0603a74978a134f63533859be5",
      "8bdb8bfae31d4f4cb6c3b0bf43120eed",
      "39a68d9a5c274e2dafaa2d1f86eea768",
      "d0cfe5dacdfc431a91b4c4741123e2d0",
      "e7f1e1a14bb740d18827dd78bbe7b2e3",
      "3fda06f905b445a488efdd2dd08c0939",
      "2bc341a780f7498ba9cd475468841bb5",
      "d7218475e23b420a8c03d00ca4ab8718",
      "a694abaf765f4d1b82fa0138e59c6793"
     ]
    },
    "id": "Vnt283M5O-qc",
    "outputId": "d2843a73-3ad5-4daa-8d1e-a58de7aa2bb0"
   },
   "outputs": [],
   "source": [
    "response = basic_rag_pipeline.run({\"text_embedder\": {\"text\": question}, \"prompt_builder\": {\"question\": question}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some movies by Ridley Scott include Gladiator, Alien, Prometheus, Blade Runner, American Gangster, Black Hawk Down, Kingdom of Heaven, Robin Hood, Hannibal, Body of Lies, Matchstick Men, The Counselor, A Good Year, G.I. Jane, Legend, Black Rain, White Squall, and The Duellists.\n"
     ]
    }
   ],
   "source": [
    "print(response['llm'][\"replies\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
